{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7ef7bee-3a6f-4b75-b6a3-6f1b245e9358",
   "metadata": {},
   "source": [
    "## Trying out DINOv3\n",
    "\n",
    "The purpose of this notebook is to fine-tune a DINOv3 model based off of a dataset of 64x64x4 chips and their corresponding chms from g-liht. \n",
    "\n",
    "Uses DEV kernel\n",
    "mjf 11/25/2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fdd0cd-0ca3-461d-9f88-c9e16d7194b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "# Set environment variables and warnings\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['NUMBA_NUM_THREADS'] = '1'\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"torch.nn.parallel.parallel_apply\")\n",
    "\n",
    "# Third-party scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from osgeo import gdal\n",
    "from tiler import Tiler, Merger\n",
    "\n",
    "# PyTorch and related\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Computer vision and image processing\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "# Hugging Face\n",
    "from transformers import AutoModel\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import LinearSegmentedColormap, ListedColormap, BoundaryNorm\n",
    "from plotnine import *\n",
    "\n",
    "# Progress bars\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ae5efd-f341-445f-af36-d15acc2afadc",
   "metadata": {},
   "source": [
    "## 1. Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb42e637-35ac-4a97-a679-4cd6c269fc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Custom loss block\n",
    "class AsymmetricMSELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom MSE Loss that penalizes underestimation more heavily than overestimation.\n",
    "    \n",
    "    For each pixel:\n",
    "    - If predicted < actual: loss = underestimation_penalty * (predicted - actual)^2\n",
    "    - If predicted >= actual: loss = (predicted - actual)^2\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    underestimation_penalty : float\n",
    "        Multiplier for underestimation errors (default: 2.0)\n",
    "    reduction : str\n",
    "        Specifies the reduction to apply: 'none' | 'mean' | 'sum' (default: 'mean')\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, underestimation_penalty=2.0, reduction='mean'):\n",
    "        super(AsymmetricMSELoss, self).__init__()\n",
    "        self.underestimation_penalty = underestimation_penalty\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(self, predicted, target):\n",
    "        \"\"\"\n",
    "        Forward pass of the asymmetric MSE loss.\n",
    "        \n",
    "        Args:\n",
    "            predicted (torch.Tensor): Predicted values\n",
    "            target (torch.Tensor): Ground truth values\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Loss value\n",
    "        \"\"\"\n",
    "        # Calculate standard MSE\n",
    "        squared_errors = (predicted - target) ** 2\n",
    "        \n",
    "        # Create mask for underestimation (predicted < target)\n",
    "        underestimation_mask = predicted < target\n",
    "        \n",
    "        # Apply penalty to underestimation errors\n",
    "        penalty_weights = torch.ones_like(squared_errors)\n",
    "        penalty_weights[underestimation_mask] = self.underestimation_penalty\n",
    "        \n",
    "        # Apply weights to squared errors\n",
    "        weighted_errors = squared_errors * penalty_weights\n",
    "        \n",
    "        # Apply reduction\n",
    "        if self.reduction == 'mean':\n",
    "            return weighted_errors.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return weighted_errors.sum()\n",
    "        elif self.reduction == 'none':\n",
    "            return weighted_errors\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid reduction mode: {self.reduction}\")\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"AsymmetricMSELoss(underestimation_penalty={self.underestimation_penalty}, reduction='{self.reduction}')\"\n",
    "\n",
    "class AsymmetricFocalMSELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Asymmetric Focal MSE Loss that:\n",
    "    1. Penalizes underestimation more heavily than overestimation\n",
    "    2. Uses focal weighting to focus more on hard examples (larger errors)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    underestimation_penalty : float\n",
    "        Multiplier for underestimation errors (default: 2.0)\n",
    "    focal_gamma : float\n",
    "        Focusing parameter - higher values increase focus on large errors (default: 2.0)\n",
    "    height_threshold : float\n",
    "        Height above which to increase focus (optional, set to None to disable)\n",
    "    threshold_gamma : float\n",
    "        Additional gamma for samples above height_threshold (default: 1.0)\n",
    "    reduction : str\n",
    "        Specifies the reduction to apply: 'none' | 'mean' | 'sum' (default: 'mean')\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, underestimation_penalty=2.0, focal_gamma=2.0, \n",
    "                 height_threshold=None, threshold_gamma=1.0, reduction='mean'):\n",
    "        super(AsymmetricFocalMSELoss, self).__init__()\n",
    "        self.underestimation_penalty = underestimation_penalty\n",
    "        self.focal_gamma = focal_gamma\n",
    "        self.height_threshold = height_threshold\n",
    "        self.threshold_gamma = threshold_gamma\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(self, predicted, target):\n",
    "        # Base errors\n",
    "        errors = torch.abs(predicted - target)\n",
    "        squared_errors = errors ** 2\n",
    "        \n",
    "        # Create mask for underestimation (predicted < target)\n",
    "        underestimation_mask = predicted < target\n",
    "        \n",
    "        # Apply asymmetric penalty\n",
    "        penalty_weights = torch.ones_like(squared_errors)\n",
    "        penalty_weights[underestimation_mask] = self.underestimation_penalty\n",
    "        \n",
    "        # Calculate focal weights based on error magnitude\n",
    "        # This emphasizes samples with larger errors\n",
    "        focal_weights = errors ** self.focal_gamma\n",
    "        \n",
    "        # If using a height threshold, apply additional focus on tall trees\n",
    "        if self.height_threshold is not None:\n",
    "            tall_vegetation_mask = target > self.height_threshold\n",
    "            # Add extra focus on tall vegetation with large errors\n",
    "            focal_weights[tall_vegetation_mask] = focal_weights[tall_vegetation_mask] * \\\n",
    "                (errors[tall_vegetation_mask] ** self.threshold_gamma)\n",
    "        \n",
    "        # Apply both weights to squared errors\n",
    "        weighted_errors = squared_errors * penalty_weights * focal_weights\n",
    "        \n",
    "        # Apply reduction\n",
    "        if self.reduction == 'mean':\n",
    "            return weighted_errors.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return weighted_errors.sum()\n",
    "        elif self.reduction == 'none':\n",
    "            return weighted_errors\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid reduction mode: {self.reduction}\")\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return (f\"AsymmetricFocalMSELoss(underestimation_penalty={self.underestimation_penalty}, \"\n",
    "                f\"focal_gamma={self.focal_gamma}, height_threshold={self.height_threshold}, \"\n",
    "                f\"threshold_gamma={self.threshold_gamma}, reduction='{self.reduction}')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501f1cbb-1278-47b8-9a78-96e77d28495a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CONFIGURATION CELL - Modify parameters here\n",
    "# ====================================================================\n",
    "\n",
    "# Dataset Configuration\n",
    "\n",
    "DATA_CONFIG = {\n",
    "    'data_name': 'chm_npy_dataset',\n",
    "    'image_dir': '/explore/nobackup/people/mmacande/srlite/chm_model/20231014_chm/train_merged_nodtm_npy/images/',\n",
    "    'label_dir': '/explore/nobackup/people/mmacande/srlite/chm_model/20231014_chm/train_merged_nodtm_npy/labels/',\n",
    "    'stats_path': '/explore/nobackup/people/mfrost2/projects/boreal_chm_dino/numpy_stats/',  \n",
    "    'np_stats': 'maxmin_ak_100k_both_nrg_final',  # Or whatever stats you're using\n",
    "    'nir_min': 0,    \n",
    "    'nir_max': 7142,\n",
    "    'red_min': 0,\n",
    "    'red_max': 5893,\n",
    "    'green_min': 0,\n",
    "    'green_max': 5387,\n",
    "}\n",
    "\n",
    "# Load means and standard deviations based on config\n",
    "DATA_CONFIG['means'] = np.load(f\"{DATA_CONFIG['stats_path']}channel_means_{DATA_CONFIG['np_stats']}.npy\")\n",
    "DATA_CONFIG['stds'] = np.load(f\"{DATA_CONFIG['stats_path']}channel_stds_{DATA_CONFIG['np_stats']}.npy\")\n",
    "DATA_CONFIG['input_bands'] = DATA_CONFIG['np_stats'][-3:]  # Extract band info from np_stats name\n",
    "\n",
    "# Print configuration info\n",
    "#print(f\"Dataset: {DATA_CONFIG['data_name']}\")\n",
    "print(f\"Input bands: {DATA_CONFIG['input_bands']}\")\n",
    "print(f\"Channel means: {DATA_CONFIG['means']}\")\n",
    "print(f\"Channel stds: {DATA_CONFIG['stds']}\")\n",
    "\n",
    "# Training Configuration  \n",
    "TRAINING_CONFIG = {\n",
    "    'n_epochs': 100,\n",
    "    'patience': 10,\n",
    "    'lr': 5e-5,\n",
    "    'loss_criterion': AsymmetricMSELoss(underestimation_penalty=3.0), #AsymmetricFocalMSELoss(underestimation_penalty=1.5,  focal_gamma=.5, height_threshold=10.0, threshold_gamma=0.5)        \n",
    "    # Options: nn.MSELoss(),  nn.L1Loss(), nn.SmoothL1Loss(), AsymmetricMSELoss()\n",
    "    'hf_token': \"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\",\n",
    "    'gradient_clip_norm': 1.0,\n",
    "    'weight_decay': 1e-3, #1e-4,\n",
    "    'scheduler_patience': 3,\n",
    "    'scheduler_factor': 0.5,\n",
    "    'dropout_rate': 0.2\n",
    "}\n",
    "\n",
    "\n",
    "# Model Configuration - CHANGE 'CURRENT_MODEL' TO SWITCH MODELS\n",
    "MODEL_CONFIGS = {\n",
    "    'large': {\n",
    "        'model_name': 'facebook/dinov3-vitl16-pretrain-sat493m',\n",
    "        'description': 'DINOv3-Large (1024 dim, ~300M params)',\n",
    "        'base_batch_size': 16,\n",
    "        'memory_efficient': True\n",
    "    },\n",
    "    '7b': {\n",
    "        'model_name': 'facebook/dinov3-vit7b16-pretrain-sat493m', \n",
    "        'description': 'DINOv3-7B (4096 dim, ~7B params)',\n",
    "        'base_batch_size': 4,  # Smaller batch size for memory\n",
    "        'memory_efficient': False\n",
    "    }\n",
    "}\n",
    "\n",
    "# SELECT WHICH MODEL TO USE HERE\n",
    "CURRENT_MODEL = 'large'  # Options: 'large' or '7b'\n",
    "\n",
    "# ====================================================================\n",
    "# Derived configurations (automatically set based on above)\n",
    "# ====================================================================\n",
    "config = MODEL_CONFIGS[CURRENT_MODEL]\n",
    "input_bands = DATA_CONFIG['np_stats'][-3:]\n",
    "#dataset_path = f\"{DATA_CONFIG['base_path']}{DATA_CONFIG['data_name']}_dataset\"\n",
    "\n",
    "# Print configuration summary\n",
    "print(\"=== CONFIGURATION SUMMARY ===\")\n",
    "#print(f\"Dataset: {DATA_CONFIG['data_name']}\")\n",
    "print(f\"Input bands: {input_bands}\")\n",
    "print(f\"Selected model: {config['description']}\")\n",
    "print(f\"Model name: {config['model_name']}\")\n",
    "print(f\"Training epochs: {TRAINING_CONFIG['n_epochs']}\")\n",
    "print(f\"Learning rate: {TRAINING_CONFIG['lr']}\")\n",
    "print(f\"Loss function: {type(TRAINING_CONFIG['loss_criterion']).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a325742-f7b0-44f7-9765-e864f88b8b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define CHM colormap\n",
    "def create_custom_binned_colormap(colors, vmax=35):\n",
    "    \"\"\"\n",
    "    Create a colormap with custom bins using ListedColormap and BoundaryNorm.\n",
    "    This avoids the LinearSegmentedColormap position issues.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the boundaries for each color bin\n",
    "    boundaries = [0, 0.001, .5, 1, 2, 3, 5, 10, vmax]\n",
    "    \n",
    "    # Create a ListedColormap (simpler than LinearSegmentedColormap)\n",
    "    forest_ht_cmap = ListedColormap(colors)\n",
    "    \n",
    "    # Create a BoundaryNorm for discrete bins\n",
    "    norm = BoundaryNorm(boundaries, len(colors))\n",
    "    \n",
    "    return forest_ht_cmap, norm, boundaries\n",
    "\n",
    "# Your original colors\n",
    "colors = ['#636363','#fc8d59','#fee08b','#ffffbf',\n",
    "          '#d9ef8b','#91cf60','#1a9850','#005a32']\n",
    "\n",
    "# Create the custom colormap\n",
    "forest_ht_cmap, forest_ht_norm, boundaries = create_custom_binned_colormap(colors, vmax=35)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa19d667-90e6-4064-a671-5eec0e2b7e88",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75485235-2446-434a-956f-bebf99f19878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# DATA PREPROCESSING AND DATASET PIPELINE\n",
    "# ====================================================================\n",
    "\n",
    "# Define transforms for your images\n",
    "def create_transform(means=None, stds=None):\n",
    "    \"\"\"Create image transformation pipeline with proper normalization\"\"\"\n",
    "    # No transforms needed since normalization is handled in the dataset\n",
    "    return None\n",
    "\n",
    "class NPYDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None, means=None, stds=None, skip_zero_chm=True):\n",
    "        \"\"\"\n",
    "        Custom dataset to load 4-band .npy images and corresponding .npy labels\n",
    "        Extracts NIR, Red, Green channels (bands 3, 2, 1) from 4-band images\n",
    "        \n",
    "        Args:\n",
    "            image_dir: Path to folder containing .npy image files\n",
    "            label_dir: Path to folder containing .npy label files\n",
    "            transform: Optional transforms to apply\n",
    "            means: Channel means for normalization [NIR, Red, Green] (after min-max)\n",
    "            stds: Channel standard deviations for normalization [NIR, Red, Green] (after min-max)\n",
    "            skip_zero_chm: If True, skip files where all CHM pixels are <= 0\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.means = means\n",
    "        self.stds = stds\n",
    "        self.skip_zero_chm = skip_zero_chm\n",
    "        \n",
    "        # Channel mapping for 4-band images (Blue, Green, Red, NIR)\n",
    "        # We want NIR, Red, Green which are bands 3, 2, 1 (0-indexed)\n",
    "        self.channel_indices = [3, 2, 1]  # NIR, Red, Green\n",
    "        \n",
    "        # Get all .npy image files\n",
    "        self.image_files = glob.glob(os.path.join(image_dir, \"*.npy\"))\n",
    "        self.image_files.sort()  # Ensure consistent ordering\n",
    "        \n",
    "        # Create mapping of image files to label files\n",
    "        self.valid_pairs = []\n",
    "        skipped_count = 0\n",
    "        \n",
    "        for img_path in self.image_files:\n",
    "            base_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "            label_path = os.path.join(label_dir, f\"{base_name}.npy\")\n",
    "            \n",
    "            if os.path.exists(label_path):\n",
    "                # Check if we should skip files with all CHM <= 0\n",
    "                if self.skip_zero_chm:\n",
    "                    try:\n",
    "                        label = np.load(label_path)\n",
    "                        if len(label.shape) > 2:\n",
    "                            label = np.squeeze(label)\n",
    "                        \n",
    "                        # Skip if all pixels are <= 0\n",
    "                        if np.all(label <= 0):\n",
    "                            skipped_count += 1\n",
    "                            continue\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error checking label {label_path}: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                self.valid_pairs.append((img_path, label_path))\n",
    "            else:\n",
    "                print(f\"Warning: No label found for {img_path}\")\n",
    "        \n",
    "        print(f\"Dataset contains {len(self.valid_pairs)} samples\")\n",
    "        if self.skip_zero_chm:\n",
    "            print(f\"Skipped {skipped_count} files with all CHM pixels <= 0\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.valid_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label_path = self.valid_pairs[idx]\n",
    "        \n",
    "        # Load 4-band image (.npy file)\n",
    "        try:\n",
    "            image = np.load(img_path)\n",
    "            \n",
    "            # Handle different possible shapes for 4-band images\n",
    "            if len(image.shape) == 3:\n",
    "                if image.shape[0] == 4:\n",
    "                    # Shape is (4, H, W) - channels first\n",
    "                    nrg_image = image[self.channel_indices]  # Shape: (3, H, W)\n",
    "                    nrg_image = np.transpose(nrg_image, (1, 2, 0))  # Convert to (H, W, 3)\n",
    "                elif image.shape[-1] == 4:\n",
    "                    # Shape is (H, W, 4) - channels last\n",
    "                    nrg_image = image[:, :, self.channel_indices]  # Shape: (H, W, 3)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected image shape: {image.shape}\")\n",
    "            else:\n",
    "                raise ValueError(f\"Expected 3D image array, got shape: {image.shape}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            nrg_image = np.zeros((64, 64, 3), dtype=np.float32)\n",
    "        \n",
    "        # Load label (.npy file)\n",
    "        try:\n",
    "            label = np.load(label_path)\n",
    "            if len(label.shape) > 2:\n",
    "                label = np.squeeze(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading label {label_path}: {e}\")\n",
    "            label = np.zeros((64, 64), dtype=np.float32)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        image = torch.from_numpy(nrg_image.copy()).float()\n",
    "        label = torch.from_numpy(label.copy()).float()\n",
    "        \n",
    "        # STEP 1: Min-max normalization to [0, 1] range\n",
    "        # NIR channel (index 0)\n",
    "        image[:, :, 0] = (image[:, :, 0] - DATA_CONFIG['nir_min']) / (DATA_CONFIG['nir_max'] - DATA_CONFIG['nir_min'])\n",
    "        image[:, :, 0] = torch.clamp(image[:, :, 0], 0, 1)\n",
    "        \n",
    "        # Red channel (index 1) \n",
    "        image[:, :, 1] = (image[:, :, 1] - DATA_CONFIG['red_min']) / (DATA_CONFIG['red_max'] - DATA_CONFIG['red_min'])\n",
    "        image[:, :, 1] = torch.clamp(image[:, :, 1], 0, 1)\n",
    "        \n",
    "        # Green channel (index 2)\n",
    "        image[:, :, 2] = (image[:, :, 2] - DATA_CONFIG['green_min']) / (DATA_CONFIG['green_max'] - DATA_CONFIG['green_min'])\n",
    "        image[:, :, 2] = torch.clamp(image[:, :, 2], 0, 1)\n",
    "        \n",
    "        # STEP 2: Z-score normalization using dataset statistics\n",
    "        if self.means is not None and self.stds is not None:\n",
    "            # NIR channel (index 0)\n",
    "            image[:, :, 0] = (image[:, :, 0] - self.means[0]) / self.stds[0]\n",
    "            # Red channel (index 1) \n",
    "            image[:, :, 1] = (image[:, :, 1] - self.means[1]) / self.stds[1]\n",
    "            # Green channel (index 2)\n",
    "            image[:, :, 2] = (image[:, :, 2] - self.means[2]) / self.stds[2]\n",
    "        \n",
    "        # Convert image to (C, H, W) format for PyTorch\n",
    "        image = image.permute(2, 0, 1)\n",
    "        \n",
    "        # Apply transforms if provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "def create_flexible_dataloaders(train_dataset, test_dataset, model_config='large'):\n",
    "    \"\"\"Create dataloaders optimized for the selected model\"\"\"\n",
    "    \n",
    "    config = MODEL_CONFIGS[model_config]\n",
    "    num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "    \n",
    "    # Use model-specific base batch size\n",
    "    base_batch_size = config['base_batch_size']\n",
    "    train_batch_size = base_batch_size * max(1, num_gpus)\n",
    "    test_batch_size = base_batch_size * max(1, num_gpus)\n",
    "    \n",
    "    # Adjust workers based on model size\n",
    "    num_workers = 4 * num_gpus if config['memory_efficient'] else 2 * num_gpus\n",
    "    \n",
    "    print(f\"DataLoader setup for {config['description']}:\")\n",
    "    print(f\"  Number of GPUs: {num_gpus}\")\n",
    "    print(f\"  Base batch size: {base_batch_size}\")\n",
    "    print(f\"  Train batch size: {train_batch_size}\")\n",
    "    print(f\"  Test batch size: {test_batch_size}\")\n",
    "    print(f\"  Workers: {num_workers}\")\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=train_batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=test_batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "# ====================================================================\n",
    "# DATA LOADING AND PREPROCESSING\n",
    "# ====================================================================\n",
    "\n",
    "# Create transforms\n",
    "transform = create_transform(means=DATA_CONFIG.get('means'), stds=DATA_CONFIG.get('stds'))\n",
    "\n",
    "# Create the full dataset from your .npy files with CHM filtering\n",
    "full_dataset = NPYDataset(\n",
    "    image_dir=DATA_CONFIG['image_dir'],\n",
    "    label_dir=DATA_CONFIG['label_dir'],\n",
    "    transform=transform,\n",
    "    means=DATA_CONFIG.get('means'),\n",
    "    stds=DATA_CONFIG.get('stds'),\n",
    "    skip_zero_chm=True  # This will skip files where all CHM pixels are <= 0\n",
    ")\n",
    "\n",
    "print(f\"Total dataset size: {len(full_dataset)}\")\n",
    "\n",
    "# Split dataset into train and test (80/20 split)\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(\n",
    "    full_dataset, \n",
    "    [train_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)  # For reproducible splits\n",
    ")\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Create optimized data loaders based on model configuration\n",
    "train_loader, test_loader = create_flexible_dataloaders(\n",
    "    train_dataset, \n",
    "    test_dataset, \n",
    "    model_config=CURRENT_MODEL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a003010d-a0c8-4efc-8f38-a84e3a18c6a3",
   "metadata": {},
   "source": [
    "## 3. Preview dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee8ce71-ad8c-4484-ae81-0b55f749c507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Visualize raw data (before normalization)\n",
    "def visualize_raw_image_and_chm(dataset, idx=0):\n",
    "    \"\"\"\n",
    "    Visualize raw image and CHM data (before normalization) from file paths\n",
    "    \"\"\"\n",
    "    # Get the file paths for the sample\n",
    "    img_path, label_path = dataset.valid_pairs[idx]\n",
    "    \n",
    "    print(f\"Loading raw files:\")\n",
    "    print(f\"Image: {os.path.basename(img_path)}\")\n",
    "    print(f\"Label: {os.path.basename(label_path)}\")\n",
    "    \n",
    "    # Load raw image\n",
    "    raw_image = np.load(img_path)\n",
    "    \n",
    "    # Extract NIR, Red, Green channels\n",
    "    if raw_image.shape[0] == 4:\n",
    "        # Shape is (4, H, W) - channels first\n",
    "        nrg_image = raw_image[[3, 2, 1]]  # NIR, Red, Green\n",
    "        nrg_image = np.transpose(nrg_image, (1, 2, 0))  # Convert to (H, W, 3)\n",
    "    elif raw_image.shape[-1] == 4:\n",
    "        # Shape is (H, W, 4) - channels last\n",
    "        nrg_image = raw_image[:, :, [3, 2, 1]]  # NIR, Red, Green\n",
    "    \n",
    "    # Load raw CHM\n",
    "    raw_chm = np.load(label_path)\n",
    "    if len(raw_chm.shape) > 2:\n",
    "        raw_chm = np.squeeze(raw_chm)\n",
    "    \n",
    "    print(f\"Raw image shape: {nrg_image.shape}\")\n",
    "    print(f\"Raw CHM shape: {raw_chm.shape}\")\n",
    "    print(f\"Raw image range: {nrg_image.min():.1f} to {nrg_image.max():.1f}\")\n",
    "    print(f\"Raw CHM range: {raw_chm.min():.3f} to {raw_chm.max():.3f}\")\n",
    "    \n",
    "    # Apply min-max stretching for visualization\n",
    "    stretched_image = np.zeros_like(nrg_image, dtype=np.float32)\n",
    "    \n",
    "    # NIR channel\n",
    "    stretched_image[:, :, 0] = (nrg_image[:, :, 0] - DATA_CONFIG['nir_min']) / (DATA_CONFIG['nir_max'] - DATA_CONFIG['nir_min'])\n",
    "    # Red channel  \n",
    "    stretched_image[:, :, 1] = (nrg_image[:, :, 1] - DATA_CONFIG['red_min']) / (DATA_CONFIG['red_max'] - DATA_CONFIG['red_min'])\n",
    "    # Green channel\n",
    "    stretched_image[:, :, 2] = (nrg_image[:, :, 2] - DATA_CONFIG['green_min']) / (DATA_CONFIG['green_max'] - DATA_CONFIG['green_min'])\n",
    "    \n",
    "    stretched_image = np.clip(stretched_image, 0, 1)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Display stretched image\n",
    "    ax1.imshow(stretched_image)\n",
    "    ax1.set_title(f'Raw NIR-Red-Green Image (Sample {idx})')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Display CHM\n",
    "    chm_plot = ax2.imshow(raw_chm, cmap=forest_ht_cmap, norm=forest_ht_norm)\n",
    "    ax2.set_title(f'Raw Canopy Height Model (Sample {idx})')\n",
    "    ax2.axis('off')\n",
    "    plt.colorbar(chm_plot, ax=ax2, label='Height (m)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2f354e-35bb-45a6-a097-254ae6833e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize raw data (before any processing)\n",
    "print(\"\\n=== Visualizing Raw Data ===\")\n",
    "visualize_raw_image_and_chm(full_dataset, idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f0af24-0cbd-469a-bf6e-53c421024524",
   "metadata": {},
   "source": [
    "## 4. Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c9fb1f-c16f-47f3-8109-4ffba5826c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make DepthHead for fine-tuning, with attention, Leaky RelU, and extra layers\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    \"\"\"Channel attention mechanism to focus on important features\"\"\"\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(channels, channels//reduction, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(channels//reduction, channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x * self.attention(x)\n",
    "\n",
    "class DINOv3DepthHead(nn.Module):\n",
    "    def __init__(self, model_name=\"facebook/dinov3-vitl16-pretrain-sat493m\", \n",
    "             freeze_backbone=True, output_channels=1, token=None, input_bands='rgb',\n",
    "             training_config=None):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Store training config for use in decoder\n",
    "        self.training_config = training_config or {}\n",
    "        self.dropout_rate = self.training_config.get('dropout_rate', 0.1)  # Default to 0.1\n",
    "        \n",
    "        # Store output_channels for use in decoder\n",
    "        self.output_channels = output_channels\n",
    "        \n",
    "        # Store output_channels for use in decoder\n",
    "        self.output_channels = output_channels\n",
    "        \n",
    "        # Load DINOv3 backbone\n",
    "        if token:\n",
    "            self.backbone = AutoModel.from_pretrained(model_name, token=token)\n",
    "        else:\n",
    "            self.backbone = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "        # Modify input weights for NRG if specified\n",
    "        if input_bands == 'nrg':\n",
    "            self._modify_input_weights_for_nrg()\n",
    "        \n",
    "        # Get actual model dimensions dynamically\n",
    "        self._determine_model_dimensions()\n",
    "        \n",
    "        # Freeze backbone if specified\n",
    "        if freeze_backbone:\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "            self._freeze_backbone = True\n",
    "        \n",
    "        print(f\"DINOv3 DepthHead initialized:\")\n",
    "        print(f\"  Model: {model_name}\")\n",
    "        print(f\"  Input bands: {input_bands}\")\n",
    "        print(f\"  Embedding dim: {self.embed_dim}\")\n",
    "        print(f\"  Total tokens: {self.total_tokens}\")\n",
    "        print(f\"  Spatial patches: {self.num_spatial_patches}\")\n",
    "        print(f\"  Using tokens {self.spatial_start}:{self.spatial_end}\")\n",
    "        \n",
    "        # Build improved decoder\n",
    "        self.depth_head = self._build_improved_decoder()\n",
    "    \n",
    "        # Initialize the final layer - ADD THIS LINE:\n",
    "        self._initialize_output_layer(target_percentile=8.42)  # 70% - 1.72, 90% = 8.42 \n",
    "    \n",
    "    def _determine_model_dimensions(self):\n",
    "        \"\"\"Dynamically determine model dimensions by running a test forward pass\"\"\"\n",
    "        # Use 64x64 dummy input to match your actual data\n",
    "        dummy_input = torch.randn(1, 3, 64, 64)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.backbone(dummy_input)\n",
    "            features = outputs.last_hidden_state\n",
    "        \n",
    "        # Get actual dimensions\n",
    "        self.total_tokens = features.shape[1]\n",
    "        self.embed_dim = features.shape[2]\n",
    "        \n",
    "        # Calculate spatial patch info for 64x64 input with 16x16 patches\n",
    "        self.patches_per_side = 64 // 16  # = 4\n",
    "        self.num_spatial_patches = self.patches_per_side ** 2  # = 16\n",
    "        \n",
    "        # Determine token structure\n",
    "        non_spatial_tokens = self.total_tokens - self.num_spatial_patches\n",
    "        \n",
    "        if non_spatial_tokens == 1:\n",
    "            # Structure: [CLS] + [16 spatial]\n",
    "            self.spatial_start = 1\n",
    "            self.spatial_end = self.total_tokens\n",
    "        elif non_spatial_tokens == 5:\n",
    "            # Structure: [CLS] + [4 register] + [16 spatial]  \n",
    "            self.spatial_start = 5\n",
    "            self.spatial_end = self.total_tokens\n",
    "        else:\n",
    "            # Generic: take last 16 tokens as spatial\n",
    "            self.spatial_start = self.total_tokens - self.num_spatial_patches\n",
    "            self.spatial_end = self.total_tokens\n",
    "        \n",
    "        print(f\"Detected model structure:\")\n",
    "        print(f\"  Total tokens: {self.total_tokens}\")\n",
    "        print(f\"  Embedding dim: {self.embed_dim}\")\n",
    "        print(f\"  Non-spatial tokens: {non_spatial_tokens}\")\n",
    "        print(f\"  Input size: 64x64\")\n",
    "        print(f\"  Patch grid: {self.patches_per_side}x{self.patches_per_side}\")\n",
    "\n",
    "    def _modify_input_weights_for_nrg(self):\n",
    "        \"\"\"Simple weight modification for NIR-Red-Green input\"\"\"\n",
    "        print(\"Modifying input weights for NIR-Red-Green bands...\")\n",
    "        \n",
    "        # Find the first Conv2d layer with 3 input channels (this is the patch embedding)\n",
    "        for name, module in self.backbone.named_modules():\n",
    "            if isinstance(module, nn.Conv2d) and module.in_channels == 3:\n",
    "                print(f\"Found patch embedding at: {name}\")\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    original_weights = module.weight.data.clone()\n",
    "                    \n",
    "                    # NIR=Red, Red=Red, Green=Green (assuming BGR input order)\n",
    "                    new_weights = torch.zeros_like(original_weights)\n",
    "                    new_weights[:, 0, :, :] = original_weights[:, 2, :, :]  # NIR <- Red\n",
    "                    new_weights[:, 1, :, :] = original_weights[:, 2, :, :]  # Red <- Red  \n",
    "                    new_weights[:, 2, :, :] = original_weights[:, 1, :, :]  # Green <- Green\n",
    "                    \n",
    "                    module.weight.data = new_weights\n",
    "                    print(\"✅ Weights modified successfully\")\n",
    "                break\n",
    "\n",
    "    def _build_improved_decoder(self):\n",
    "        \"\"\"\n",
    "        Improved decoder adapted for 16x16 input (1x1 spatial features)\n",
    "        \"\"\"\n",
    "        \n",
    "        class ResidualBlock(nn.Module):\n",
    "            \"\"\"Residual block with LeakyReLU and attention\"\"\"\n",
    "            def __init__(self, channels, use_attention=True, dropout_rate=0.1):\n",
    "                super().__init__()\n",
    "                self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "                self.bn1 = nn.BatchNorm2d(channels)\n",
    "                self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "                self.bn2 = nn.BatchNorm2d(channels)\n",
    "                self.activation = nn.LeakyReLU(0.1, inplace=True)\n",
    "                \n",
    "                # Add dropout after activation\n",
    "                self.dropout = nn.Dropout2d(dropout_rate)\n",
    "                \n",
    "                self.attention = ChannelAttention(channels) if use_attention else nn.Identity()\n",
    "                \n",
    "            def forward(self, x):\n",
    "                residual = x\n",
    "                out = self.activation(self.bn1(self.conv1(x)))\n",
    "                out = self.dropout(out)  # Apply dropout after activation\n",
    "                out = self.bn2(self.conv2(out))\n",
    "                out = self.attention(out)\n",
    "                out += residual  # Skip connection\n",
    "                return self.activation(out)\n",
    "               \n",
    "        # Get dropout rate from training config\n",
    "        dropout_rate = self.dropout_rate\n",
    "        \n",
    "        layers = nn.ModuleList()\n",
    "        current_channels = self.embed_dim\n",
    "        \n",
    "        # Need 4 upsampling steps: 4x4 -> 8x8 -> 16x16 -> 32x32 -> 64x64\n",
    "        target_channels = [512, 256, 128, 64]  # Same number, but explicit about path\n",
    "        \n",
    "        for i, out_channels in enumerate(target_channels):\n",
    "            # Upsampling block (2x upsampling each time)\n",
    "            upsample_block = nn.Sequential(\n",
    "                nn.ConvTranspose2d(current_channels, out_channels, \n",
    "                                 kernel_size=4, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.LeakyReLU(0.1, inplace=True),\n",
    "                nn.Dropout2d(dropout_rate)  # Use config dropout rate\n",
    "            )\n",
    "            layers.append(upsample_block)\n",
    "            \n",
    "            # Residual refinement blocks with dropout\n",
    "            refinement_block = nn.Sequential(\n",
    "                ResidualBlock(out_channels, use_attention=(i >= 2), dropout_rate=dropout_rate),\n",
    "                ResidualBlock(out_channels, use_attention=(i >= 2), dropout_rate=dropout_rate)\n",
    "            )\n",
    "            layers.append(refinement_block)\n",
    "            \n",
    "            current_channels = out_channels\n",
    "\n",
    "        # Final layers - these should NOT change spatial dimensions\n",
    "        # After 4 upsampling steps, we should already be at 64x64\n",
    "        final_layers = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout2d(dropout_rate),  # Use config dropout rate\n",
    "            ChannelAttention(128),\n",
    "            \n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout2d(dropout_rate),  # Use config dropout rate\n",
    "            \n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            \n",
    "            # No dropout before final prediction layer\n",
    "            # Final prediction - should already be 64x64\n",
    "            nn.Conv2d(32, self.output_channels, kernel_size=1)\n",
    "        )\n",
    "        layers.append(final_layers)\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "    # Validate input size\n",
    "        if x.shape[-2:] != (64, 64):\n",
    "            raise ValueError(f\"Expected input size 64x64, got {x.shape[-2:]}\")\n",
    "        \n",
    "        # Get features from DINOv3\n",
    "        if hasattr(self, '_freeze_backbone'):\n",
    "            with torch.no_grad():\n",
    "                outputs = self.backbone(x)\n",
    "        else:\n",
    "            outputs = self.backbone(x)\n",
    "            \n",
    "        features = outputs.last_hidden_state\n",
    "        \n",
    "        # Extract spatial tokens (should be just 1 token for 16x16 input)\n",
    "        spatial_tokens = features[:, self.spatial_start:self.spatial_end, :]\n",
    "        batch_size = spatial_tokens.shape[0]\n",
    "        \n",
    "        # Verify we have the right number of spatial patches\n",
    "        expected_spatial_tokens = self.spatial_end - self.spatial_start\n",
    "        if spatial_tokens.shape[1] != expected_spatial_tokens:\n",
    "            raise ValueError(f\"Expected {expected_spatial_tokens} spatial tokens, got {spatial_tokens.shape[1]}\")\n",
    "        \n",
    "        # Reshape to spatial grid (4x4 for 64x64 input)  # ← CORRECTED COMMENT\n",
    "        spatial_features = spatial_tokens.transpose(1, 2).reshape(\n",
    "            batch_size, self.embed_dim, self.patches_per_side, self.patches_per_side  # 4x4\n",
    "        )\n",
    "        \n",
    "        # Pass through decoder (4x4 -> 64x64)  # ← CORRECTED COMMENT\n",
    "        depth_map = self.depth_head(spatial_features)\n",
    "        depth_map = depth_map.squeeze(1)\n",
    "\n",
    "        depth_map = torch.clamp(depth_map, min=0.0)\n",
    "        \n",
    "        return depth_map\n",
    "        \n",
    "    def _initialize_output_layer(self, target_mean=None, target_percentile=None):\n",
    "        \"\"\"\n",
    "        Initialize final layer to better predict the full range with skewed data\n",
    "        \n",
    "        Args:\n",
    "            target_mean (float, optional): The mean value of your targets\n",
    "            target_percentile (float, optional): A specific percentile value from your targets\n",
    "                                               (e.g., 0.7 would be the 70th percentile)\n",
    "        \"\"\"\n",
    "        # Find the final convolutional layer\n",
    "        final_conv = None\n",
    "        for name, module in self.named_modules():\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                final_conv = module\n",
    "        \n",
    "        if final_conv is None:\n",
    "            print(\"Warning: Could not find final convolutional layer for initialization\")\n",
    "            return\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            # Initialize weights with a slightly higher standard deviation\n",
    "            # to encourage more diverse predictions\n",
    "            nn.init.normal_(final_conv.weight, mean=0.0, std=0.02)\n",
    "            \n",
    "            # Initialize the bias to address the skew\n",
    "            if final_conv.bias is not None:\n",
    "                # If you've calculated statistics from your dataset:\n",
    "                if target_mean is not None:\n",
    "                    final_conv.bias.fill_(target_mean)\n",
    "                elif target_percentile is not None:\n",
    "                    final_conv.bias.fill_(target_percentile)\n",
    "                else:\n",
    "                    # Default initialization - adjust based on your data's scale\n",
    "                    # This value should be in the same scale as your target values\n",
    "                    final_conv.bias.fill_(3.0)  # Example value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c471010c-bf43-4f2d-8dac-94847d474c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dinov3_training_flexible(\n",
    "    train_loader, \n",
    "    test_loader,\n",
    "    data_config,    # NEW: Pass in data configuration\n",
    "    training_config,  # NEW: Pass in training configuration\n",
    "    model_config='large'  # Keep this as a simple string parameter\n",
    "):\n",
    "    \"\"\"\n",
    "    Training function with flexible model selection and configuration\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_loader : DataLoader\n",
    "        PyTorch DataLoader for training data\n",
    "    test_loader : DataLoader\n",
    "        PyTorch DataLoader for validation/test data\n",
    "    data_config : dict\n",
    "        Configuration for dataset details\n",
    "    training_config : dict\n",
    "        Configuration for training parameters\n",
    "    model_config : str\n",
    "        Model configuration key ('large' or '7b')\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get model configuration\n",
    "    config = MODEL_CONFIGS[model_config]\n",
    "    \n",
    "    # Extract required variables from configs\n",
    "    loss_criterion = training_config['loss_criterion']\n",
    "    base_lr = training_config['lr']\n",
    "    weight_decay = training_config.get('weight_decay', 1e-4)\n",
    "    patience = training_config.get('patience', 10)\n",
    "    num_epochs = training_config.get('n_epochs', 30)\n",
    "    hf_token = training_config.get('hf_token', None)\n",
    "    gradient_clip_norm = training_config.get('gradient_clip_norm', 1.0)\n",
    "    \n",
    "    # Get data-specific information\n",
    "    data_name = data_config['data_name']\n",
    "    input_bands = data_config.get('np_stats')[-3:]  # Extract from data config\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(f\"STARTING DINOV3 MULTI-GPU TRAINING\")\n",
    "    print(f\"Model: {config['description']}\")\n",
    "    print(\"=\"*60)\n",
    "       \n",
    "    # Check available GPUs\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        print(f\"Found {num_gpus} GPUs available:\")\n",
    "        for i in range(num_gpus):\n",
    "            print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print(\"No GPUs available, using CPU\")\n",
    "        return None, [], []\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Create model with selected configuration\n",
    "    model = DINOv3DepthHead(\n",
    "        model_name=config['model_name'],\n",
    "        freeze_backbone=True,\n",
    "        token=hf_token,\n",
    "        input_bands=input_bands\n",
    "    )\n",
    "    \n",
    "    # Move to primary GPU first\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Wrap with DataParallel for multi-GPU training\n",
    "    if num_gpus > 1:\n",
    "        print(f\"Using DataParallel across {num_gpus} GPUs\")\n",
    "        model = nn.DataParallel(model)\n",
    "        effective_batch_size = train_loader.batch_size * num_gpus\n",
    "        print(f\"Effective batch size: {effective_batch_size}\")\n",
    "    \n",
    "    # Training setup - adjust learning rate based on model size\n",
    "    criterion = loss_criterion\n",
    "    model_params = model.module.depth_head.parameters() if hasattr(model, 'module') else model.depth_head.parameters()\n",
    "    \n",
    "    # Adjust learning rate based on model size\n",
    "    adjusted_lr = base_lr if model_config == 'large' else base_lr * 0.1\n",
    "    optimizer = torch.optim.AdamW(model_params, lr=adjusted_lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # Smaller learning rate for larger model\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, min_lr=1e-6)\n",
    "    \n",
    "    print(f\"Training setup:\")\n",
    "    print(f\"  Learning rate: {base_lr}\")\n",
    "    print(f\"  Patience: {patience}\")\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs} (Patience: {patience_counter}/{patience})\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_mae = 0.0\n",
    "        \n",
    "        for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_mae += nn.L1Loss()(outputs, targets).item()\n",
    "            \n",
    "            # if batch_idx % 100 == 0:\n",
    "            #     print(f\"  Batch {batch_idx}/{len(train_loader)}: Loss = {loss.item():.4f}\")\n",
    "            if batch_idx % 500 == 0:\n",
    "                with torch.no_grad():\n",
    "  \n",
    "                    print(f\"  Batch {batch_idx}/{len(train_loader)}: Loss = {loss.item():.4f}\")\n",
    "            \n",
    "            # More frequent memory cleanup for 7B model\n",
    "            if not config['memory_efficient'] and batch_idx % 25 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_train_mae = train_mae / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_mae = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, targets in test_loader:\n",
    "                images, targets = images.to(device), targets.to(device)\n",
    "                outputs = model(images)\n",
    "                \n",
    "                val_loss += criterion(outputs, targets).item()\n",
    "                val_mae += nn.L1Loss()(outputs, targets).item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(test_loader)\n",
    "        avg_val_mae = val_mae / len(test_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        #####################################\n",
    "        epoch_pred_ranges = []\n",
    "        epoch_target_ranges = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Sample a few batches for range analysis\n",
    "            sample_batches = min(5, len(test_loader))  # Sample 5 batches max\n",
    "            for i, (images, targets) in enumerate(test_loader):\n",
    "                if i >= sample_batches:\n",
    "                    break\n",
    "                    \n",
    "                images, targets = images.to(device), targets.to(device)\n",
    "                outputs = model(images)\n",
    "                \n",
    "                epoch_pred_ranges.append([outputs.min().item(), outputs.max().item()])\n",
    "                epoch_target_ranges.append([targets.min().item(), targets.max().item()])\n",
    "        \n",
    "        # Calculate epoch statistics\n",
    "        avg_pred_min = np.mean([r[0] for r in epoch_pred_ranges])\n",
    "        avg_pred_max = np.mean([r[1] for r in epoch_pred_ranges])\n",
    "        avg_target_min = np.mean([r[0] for r in epoch_target_ranges])\n",
    "        avg_target_max = np.mean([r[1] for r in epoch_target_ranges])\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1} Range Analysis:\")\n",
    "        print(f\"  Predictions: [{avg_pred_min:.1f}, {avg_pred_max:.1f}] (range: {avg_pred_max - avg_pred_min:.1f})\")\n",
    "        print(f\"  Ground Truth: [{avg_target_min:.1f}, {avg_target_max:.1f}] (range: {avg_target_max - avg_target_min:.1f})\")\n",
    "        \n",
    "        scheduler.step(avg_val_loss) \n",
    "        \n",
    "        # Check for best model and save if improved\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Save BEST model with config info\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model_to_save.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "                'val_mae': avg_val_mae,\n",
    "                'train_losses': train_losses,      # COMPLETE history (this was missing!)\n",
    "                'val_losses': val_losses,          # COMPLETE history (this was missing!)\n",
    "                'model_config': {\n",
    "                    'model_type': model_config,\n",
    "                    'model_name': config['model_name'],\n",
    "                    'embed_dim': model_to_save.embed_dim,\n",
    "                    'total_tokens': model_to_save.total_tokens,\n",
    "                    'description': config['description']\n",
    "                }\n",
    "            }, f'best_dinov3_{model_config}_{data_name}.pth')\n",
    "            \n",
    "            improvement = \"⭐ NEW BEST\"\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            improvement = f\"No improvement ({patience_counter}/{patience})\"\n",
    "        \n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f} | Train MAE: {avg_train_mae:.4f}\")\n",
    "        print(f\"  Val Loss:   {avg_val_loss:.4f} | Val MAE:   {avg_val_mae:.4f}\")\n",
    "        print(f\"  Val RMSE:   {np.sqrt(avg_val_loss):.4f}\")\n",
    "        print(f\"  Status: {improvement}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\n🛑 EARLY STOPPING at epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        # Memory cleanup\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Save LAST model (regardless of performance)\n",
    "    print(f\"\\n💾 Saving final model from epoch {epoch+1}\")\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model_to_save.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': avg_train_loss,\n",
    "        'val_loss': avg_val_loss,\n",
    "        'val_mae': avg_val_mae,\n",
    "        'train_losses': train_losses,      \n",
    "        'val_losses': val_losses,          \n",
    "        'final_epoch': True,  # Flag to indicate this is the final model\n",
    "        'model_config': {\n",
    "            'model_type': model_config,\n",
    "            'model_name': config['model_name'],\n",
    "            'embed_dim': model_to_save.embed_dim,\n",
    "            'total_tokens': model_to_save.total_tokens,\n",
    "            'description': config['description']\n",
    "        }\n",
    "    }, f'last_dinov3_{model_config}_{data_name}.pth')\n",
    "    \n",
    "    print(f\"✅ Training complete!\")\n",
    "    print(f\"   Best model saved as: best_dinov3_{CURRENT_MODEL}_{data_name}.pth\")\n",
    "    print(f\"   Last model saved as: last_dinov3_{CURRENT_MODEL}_{data_name}.pth\")\n",
    "    print(f\"   Best validation loss: {best_val_loss:.4f}\")\n",
    "    print(f\"   Final validation loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90037c20-ac07-485f-a1b6-63d090aeba9a",
   "metadata": {},
   "source": [
    "## 5. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b278b0e-e852-4772-ac15-f8acb3df3a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create dataloaders for current model selection\n",
    "# train_loader_current, test_loader_current = create_flexible_dataloaders(\n",
    "#     train_dataset, test_dataset, model_config=CURRENT_MODEL\n",
    "# )\n",
    "\n",
    "# # Run training with selected model\n",
    "# trained_model, train_losses, val_losses = run_dinov3_training_flexible(\n",
    "#     train_loader=train_loader_current,\n",
    "#     test_loader=test_loader_current,\n",
    "#     data_config=DATA_CONFIG,           # NEW: Pass data configuration\n",
    "#     training_config=TRAINING_CONFIG,   # NEW: Pass training configuration\n",
    "#     model_config=CURRENT_MODEL         # Keep this the same\n",
    "# )\n",
    "\n",
    "\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "print(f\"Starting training at: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Create dataloaders for current model selection\n",
    "train_loader_current, test_loader_current = create_flexible_dataloaders(\n",
    "    train_dataset, test_dataset, model_config=CURRENT_MODEL\n",
    ")\n",
    "\n",
    "# # Run training with selected model\n",
    "# trained_model, train_losses, val_losses = run_dinov3_training_flexible(\n",
    "#     train_loader=train_loader_current,\n",
    "#     test_loader=test_loader_current,\n",
    "#     data_config=DATA_CONFIG,           # NEW: Pass data configuration\n",
    "#     training_config=TRAINING_CONFIG,   # NEW: Pass training configuration\n",
    "#     model_config=CURRENT_MODEL         # Keep this the same\n",
    "# )\n",
    "\n",
    "# End timer and calculate duration\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "duration_formatted = str(timedelta(seconds=int(duration)))\n",
    "\n",
    "print(f\"Training completed at: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Total training time: {duration_formatted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b62f006-7b9f-464e-bdea-a17320f0981d",
   "metadata": {},
   "source": [
    "## 6. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8097db55-a605-4158-baf7-4c6c9ebf9b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Model\n",
    "def load_flexible_model(checkpoint_path, device='cuda', input_bands='nrg'):\n",
    "    \"\"\"Load a model checkpoint regardless of which model type it is\"\"\"\n",
    "    \n",
    "    print(f\"Loading model from: {checkpoint_path}\")\n",
    "    \n",
    "    # Load checkpoint to check model type\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_config = checkpoint['model_config']  # This line was missing!\n",
    "    \n",
    "    print(f\"Loading {model_config['description']}\")\n",
    "    print(f\"  Model name: {model_config['model_name']}\")\n",
    "    print(f\"  Embedding dim: {model_config['embed_dim']}\")\n",
    "    print(f\"  Total tokens: {model_config['total_tokens']}\")\n",
    "    \n",
    "    # Create model with correct configuration\n",
    "    token = \"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\"\n",
    "    model = DINOv3DepthHead(\n",
    "        model_name=model_config['model_name'],\n",
    "        freeze_backbone=True,\n",
    "        token=token,\n",
    "        input_bands=input_bands\n",
    "    )\n",
    "    \n",
    "    # Load weights\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"✅ Model loaded successfully!\")\n",
    "    print(f\"  Best epoch: {checkpoint['epoch']}\")\n",
    "    print(f\"  Best validation loss: {checkpoint['val_loss']:.4f}\")\n",
    "    \n",
    "    return model, checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ceeea4-e2db-4943-81b7-575e51daa503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "trained_model = None\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Try to load existing model and training history\n",
    "model_filename = f'last_dinov3_{CURRENT_MODEL}_{DATA_CONFIG[\"data_name\"]}.pth'\n",
    "if model_filename in os.listdir('.'):\n",
    "    trained_model, checkpoint = load_flexible_model(model_filename)\n",
    "    \n",
    "    # Check if training history exists in checkpoint\n",
    "    if 'train_losses' in checkpoint:\n",
    "        train_losses = checkpoint['train_losses']\n",
    "    if 'val_losses' in checkpoint:\n",
    "        val_losses = checkpoint['val_losses']\n",
    "\n",
    "# Debug: Verify the data was loaded\n",
    "print(f\"Loaded {len(train_losses)} training loss values\")\n",
    "print(f\"Loaded {len(val_losses)} validation loss values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae18de36-f018-4172-be52-1b087eca66c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot results Loss and RMSE\n",
    "if trained_model is not None and len(train_losses) > 0:\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "    plt.plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "    \n",
    "    best_epoch = np.argmin(val_losses) + 1\n",
    "    plt.axvline(x=best_epoch, color='green', linestyle='--', alpha=0.7, label=f'Best epoch: {best_epoch}')\n",
    "    \n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.title(f'{MODEL_CONFIGS[CURRENT_MODEL][\"description\"]} - Training Progress')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    train_rmse = [np.sqrt(loss) for loss in train_losses]\n",
    "    val_rmse = [np.sqrt(loss) for loss in val_losses]\n",
    "    plt.plot(epochs, train_rmse, 'b-', label='Training RMSE', linewidth=2)\n",
    "    plt.plot(epochs, val_rmse, 'r-', label='Validation RMSE', linewidth=2)\n",
    "    plt.axvline(x=best_epoch, color='green', linestyle='--', alpha=0.7)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('RMSE (meters)')\n",
    "    plt.title('RMSE Progress')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nTraining Summary ({MODEL_CONFIGS[CURRENT_MODEL]['description']}):\")\n",
    "    print(f\"  Total epochs run: {len(train_losses)}\")\n",
    "    print(f\"  Best epoch: {best_epoch}\")\n",
    "    print(f\"  Best validation loss: {min(val_losses):.4f}\")\n",
    "    print(f\"  Best validation RMSE: {np.sqrt(min(val_losses)):.4f} meters\")\n",
    "    \n",
    "    if len(train_losses) < TRAINING_CONFIG['n_epochs']:\n",
    "        print(f\"  ✅ Early stopping triggered - saved {TRAINING_CONFIG['n_epochs'] - len(train_losses)} epochs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb2dbdb-e3c5-45d8-b304-b0b7de51df7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Heatmap correlation plots\n",
    "def safe_corrcoef(x, y):\n",
    "    \"\"\"Safely compute correlation coefficient with robust error handling\"\"\"\n",
    "    try:\n",
    "        # Convert to numpy arrays and ensure they're 1D\n",
    "        x = np.asarray(x).flatten()\n",
    "        y = np.asarray(y).flatten()\n",
    "        \n",
    "        # Check for valid input\n",
    "        if len(x) != len(y) or len(x) < 2:\n",
    "            return 0.0\n",
    "            \n",
    "        # Remove any NaN or infinite values\n",
    "        mask = np.isfinite(x) & np.isfinite(y)\n",
    "        if np.sum(mask) < 2:\n",
    "            return 0.0\n",
    "            \n",
    "        x_clean = x[mask]\n",
    "        y_clean = y[mask]\n",
    "        \n",
    "        # Check for zero variance\n",
    "        if np.var(x_clean) == 0 or np.var(y_clean) == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        # Manual correlation calculation to avoid BLAS issues\n",
    "        x_mean = np.mean(x_clean)\n",
    "        y_mean = np.mean(y_clean)\n",
    "        \n",
    "        numerator = np.sum((x_clean - x_mean) * (y_clean - y_mean))\n",
    "        x_var = np.sum((x_clean - x_mean)**2)\n",
    "        y_var = np.sum((y_clean - y_mean)**2)\n",
    "        \n",
    "        denominator = np.sqrt(x_var * y_var)\n",
    "        \n",
    "        if denominator == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        correlation = numerator / denominator\n",
    "        \n",
    "        # Clamp to valid range due to numerical precision\n",
    "        correlation = np.clip(correlation, -1.0, 1.0)\n",
    "        \n",
    "        return correlation\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Correlation calculation failed with error: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "def collect_predictions(model, dataloader, device, max_samples=None):\n",
    "    \"\"\"\n",
    "    Collect ground truth and predicted values from the model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_actual = []\n",
    "    all_predicted = []\n",
    "    \n",
    "    # Count total samples to process\n",
    "    total_samples = 0\n",
    "    for batch in dataloader:\n",
    "        total_samples += batch[0].shape[0]\n",
    "        if max_samples and total_samples >= max_samples:\n",
    "            break\n",
    "    \n",
    "    # Create progress bar with position and leave parameters\n",
    "    pbar = tqdm(\n",
    "        total=min(total_samples, max_samples) if max_samples else total_samples, \n",
    "        desc=\"Collecting predictions\",\n",
    "        position=0,\n",
    "        leave=True\n",
    "    )\n",
    "    \n",
    "    # Process batches\n",
    "    samples_collected = 0\n",
    "    with torch.no_grad():\n",
    "        for images, depths in dataloader:\n",
    "            batch_size = images.shape[0]\n",
    "            \n",
    "            # Skip if we've collected enough samples\n",
    "            if max_samples and samples_collected >= max_samples:\n",
    "                break\n",
    "                \n",
    "            # Process the batch\n",
    "            images = images.to(device)\n",
    "            depths = depths\n",
    "            \n",
    "            # Generate predictions\n",
    "            predictions = model(images).cpu()\n",
    "            \n",
    "            # Flatten for easy analysis (pixel-wise comparison)\n",
    "            actual_flat = depths.numpy().flatten()\n",
    "            pred_flat = predictions.numpy().flatten()\n",
    "            \n",
    "            # Filter out any invalid values before storing\n",
    "            valid_mask = np.isfinite(actual_flat) & np.isfinite(pred_flat)\n",
    "            if np.any(valid_mask):\n",
    "                all_actual.append(actual_flat[valid_mask])\n",
    "                all_predicted.append(pred_flat[valid_mask])\n",
    "            \n",
    "            # Update counts and progress\n",
    "            samples_collected += batch_size\n",
    "            pbar.update(batch_size)\n",
    "            \n",
    "    pbar.close()\n",
    "    \n",
    "    # Combine all data\n",
    "    if all_actual and all_predicted:\n",
    "        all_actual = np.concatenate(all_actual)\n",
    "        all_predicted = np.concatenate(all_predicted)\n",
    "    else:\n",
    "        print(\"Warning: No valid predictions collected!\")\n",
    "        all_actual = np.array([])\n",
    "        all_predicted = np.array([])\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'actual': all_actual,\n",
    "        'predicted': all_predicted\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_comparison_plot_matplotlib(train_df, val_df, bins=50, title='Actual vs. Predicted Height', cmap='plasma'):\n",
    "    \"\"\"\n",
    "    Create side-by-side bin2d plots using matplotlib with plasma colormap\n",
    "    and properly positioned colorbar.\n",
    "    \"\"\"\n",
    "    # Check if dataframes have data\n",
    "    if len(train_df) == 0 or len(val_df) == 0:\n",
    "        print(\"Error: Empty dataframes provided to plotting function\")\n",
    "        return None\n",
    "    \n",
    "    # Determine common range for both plots\n",
    "    min_val = min(train_df['actual'].min(), train_df['predicted'].min(), \n",
    "                 val_df['actual'].min(), val_df['predicted'].min())\n",
    "    max_val = max(train_df['actual'].max(), train_df['predicted'].max(), \n",
    "                 val_df['actual'].max(), val_df['predicted'].max())\n",
    "    \n",
    "    # Calculate metrics using safe correlation\n",
    "    train_corr = safe_corrcoef(train_df['actual'].values, train_df['predicted'].values)\n",
    "    val_corr = safe_corrcoef(val_df['actual'].values, val_df['predicted'].values)\n",
    "    \n",
    "    train_r2 = train_corr**2\n",
    "    val_r2 = val_corr**2\n",
    "    \n",
    "    train_rmse = np.sqrt(np.mean((train_df['actual'] - train_df['predicted'])**2))\n",
    "    val_rmse = np.sqrt(np.mean((val_df['actual'] - val_df['predicted'])**2))\n",
    "    \n",
    "    # Create the plot with more space for colorbar\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "    \n",
    "    try:\n",
    "        # Training data (left plot)\n",
    "        h1 = ax1.hist2d(train_df['actual'], train_df['predicted'], bins=bins, \n",
    "                       cmap=cmap, norm=plt.matplotlib.colors.LogNorm())\n",
    "        \n",
    "        ax1.plot([min_val, max_val], [min_val, max_val], 'b--', linewidth=1.5)\n",
    "        ax1.set_xlabel('Actual Height (m)', fontsize=12)\n",
    "        ax1.set_ylabel('Predicted Height (m)', fontsize=12)\n",
    "        ax1.set_title(f'Training\\nR² = {train_r2:.3f}, RMSE = {train_rmse:.3f}', fontsize=14)\n",
    "        ax1.set_xlim(min_val, max_val)\n",
    "        ax1.set_ylim(min_val, max_val)\n",
    "        ax1.set_aspect('equal')\n",
    "        ax1.grid(alpha=0.3)\n",
    "        \n",
    "        # Validation data (right plot)\n",
    "        h2 = ax2.hist2d(val_df['actual'], val_df['predicted'], bins=bins, \n",
    "                       cmap=cmap, norm=plt.matplotlib.colors.LogNorm())\n",
    "        \n",
    "        ax2.plot([min_val, max_val], [min_val, max_val], 'b--', linewidth=1.5)\n",
    "        ax2.set_xlabel('Actual Height (m)', fontsize=12)\n",
    "        ax2.set_ylabel('Predicted Height (m)', fontsize=12)\n",
    "        ax2.set_title(f'Validation\\nR² = {val_r2:.3f}, RMSE = {val_rmse:.3f}', fontsize=14)\n",
    "        ax2.set_xlim(min_val, max_val)\n",
    "        ax2.set_ylim(min_val, max_val)\n",
    "        ax2.set_aspect('equal')\n",
    "        ax2.grid(alpha=0.3)\n",
    "        \n",
    "        # Adjust layout to make room for colorbar\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Create a new axis for the colorbar with specific positioning\n",
    "        cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "        cbar = fig.colorbar(h1[3], cax=cbar_ax)\n",
    "        cbar.set_label('Count (log scale)', fontsize=12)\n",
    "        cbar.ax.tick_params(labelsize=10)\n",
    "        \n",
    "        # Add main title with adjusted positioning\n",
    "        fig.suptitle(title, fontsize=16, fontweight='bold', y=0.98)\n",
    "        \n",
    "        # Less aggressive tight_layout that respects the colorbar position\n",
    "        plt.subplots_adjust(right=0.9, top=0.9)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating plots: {e}\")\n",
    "        return None\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def evaluate_and_visualize_model(model, train_loader, test_loader, device, \n",
    "                                max_samples=10000, title='Actual vs. Predicted Height',\n",
    "                                model_name='model', data_name='dataset'):\n",
    "    \"\"\"\n",
    "    Evaluate model and create comparison visualization\n",
    "    \"\"\"\n",
    "    print(\"Collecting training predictions...\")\n",
    "    train_df = collect_predictions(model, train_loader, device, max_samples)\n",
    "    \n",
    "    print(\"Collecting validation predictions...\")\n",
    "    val_df = collect_predictions(model, test_loader, device, max_samples)\n",
    "    \n",
    "    if len(train_df) == 0 or len(val_df) == 0:\n",
    "        print(\"❌ No valid predictions collected. Check your data and model.\")\n",
    "        return None, None, None\n",
    "    \n",
    "    print(\"Creating visualization...\")\n",
    "    fig = create_comparison_plot_matplotlib(train_df, val_df, title=title, cmap='plasma')\n",
    "    \n",
    "    if fig is None:\n",
    "        print(\"❌ Failed to create visualization.\")\n",
    "        return None, train_df, val_df\n",
    "    \n",
    "    print(f\"Training samples: {len(train_df)}\")\n",
    "    print(f\"Validation samples: {len(val_df)}\")\n",
    "    \n",
    "    # Calculate and print additional metrics using safe correlation\n",
    "    train_corr = safe_corrcoef(train_df['actual'].values, train_df['predicted'].values)\n",
    "    val_corr = safe_corrcoef(val_df['actual'].values, val_df['predicted'].values)\n",
    "    \n",
    "    train_r2 = train_corr**2\n",
    "    val_r2 = val_corr**2\n",
    "    \n",
    "    train_rmse = np.sqrt(np.mean((train_df['actual'] - train_df['predicted'])**2))\n",
    "    val_rmse = np.sqrt(np.mean((val_df['actual'] - val_df['predicted'])**2))\n",
    "    \n",
    "    train_mae = np.mean(np.abs(train_df['actual'] - train_df['predicted']))\n",
    "    val_mae = np.mean(np.abs(val_df['actual'] - val_df['predicted']))\n",
    "    \n",
    "    print(f\"\\nDetailed Metrics:\")\n",
    "    print(f\"Training   - R²: {train_r2:.4f}, RMSE: {train_rmse:.4f}, MAE: {train_mae:.4f}\")\n",
    "    print(f\"Validation - R²: {val_r2:.4f}, RMSE: {val_rmse:.4f}, MAE: {val_mae:.4f}\")\n",
    "    \n",
    "    # Save the figure\n",
    "    try:\n",
    "        model_desc = f\"{model_name}_depth_estimation\"\n",
    "        filename = f'dinov3_depth_prediction_comparison_{model_desc}_{data_name}.png'\n",
    "        fig.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Plot saved as: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not save plot: {e}\")\n",
    "    \n",
    "    return fig, train_df, val_df\n",
    "\n",
    "# Now run the comprehensive evaluation\n",
    "print(\"=\"*60)\n",
    "print(\"COMPREHENSIVE MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check for model and run evaluation\n",
    "model_filename = f'best_dinov3_{CURRENT_MODEL}_{DATA_CONFIG[\"data_name\"]}.pth' \n",
    "\n",
    "if model_filename in os.listdir('.'):\n",
    "    print(f\"Loading model from checkpoint: {model_filename}\")\n",
    "    model_for_eval, checkpoint = load_flexible_model(model_filename)  # ← Fixed this line\n",
    "    \n",
    "    if model_for_eval is not None:\n",
    "        result = evaluate_and_visualize_model(\n",
    "            model=model_for_eval,\n",
    "            train_loader=train_loader_current,\n",
    "            test_loader=test_loader_current,\n",
    "            device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "            max_samples=50000,\n",
    "            title=f'DINOv3-{CURRENT_MODEL.title()} Height Estimation: Actual vs. Predicted CHM',\n",
    "            model_name=CURRENT_MODEL,  # Add this\n",
    "            data_name=DATA_CONFIG['data_name']  # Add this\n",
    "        )\n",
    "        \n",
    "        if result[0] is not None:  # fig is not None\n",
    "            fig, train_df, val_df = result\n",
    "            plt.show()\n",
    "            print(\"\\n✅ Comprehensive evaluation completed!\")\n",
    "        else:\n",
    "            print(\"❌ Evaluation failed during visualization.\")\n",
    "    else:\n",
    "        print(\"❌ Failed to load model.\")\n",
    "else:\n",
    "    print(f\"❌ No trained model found: {model_filename}\")\n",
    "    print(f\"Available .pth files: {[f for f in os.listdir('.') if f.endswith('.pth')]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cb402f-f3de-46c8-93f8-3574316947f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images\n",
    "def visualize_predictions_fixed(model, dataloader, data_config, num_samples=20, device='cuda'):\n",
    "    \"\"\"\n",
    "    Visualize model predictions with proper configuration handling\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : torch model\n",
    "        Trained model for inference\n",
    "    dataloader : DataLoader\n",
    "        Test data loader\n",
    "    data_config : dict\n",
    "        Configuration containing means, stds, and other data info\n",
    "    num_samples : int\n",
    "        Number of samples to visualize\n",
    "    device : str or torch.device\n",
    "        Device for computation\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Extract values from config\n",
    "    means = data_config['means']\n",
    "    stds = data_config['stds']\n",
    "    \n",
    "    def stretch_nrg_image_std(nrg_image, means, stds, n_std=2):\n",
    "        \"\"\"\n",
    "        Apply mean/std stretching for NRG visualization\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        nrg_image : np.ndarray\n",
    "            Image array of shape [H, W, C] where C >= 3 (NRG channels)\n",
    "        means : np.ndarray\n",
    "            Mean values for each channel\n",
    "        stds : np.ndarray  \n",
    "            Standard deviation values for each channel\n",
    "        n_std : float\n",
    "            Number of standard deviations to stretch to (default: 2)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray : Stretched image clipped to [0, 1]\n",
    "        \"\"\"\n",
    "        stretched_image = np.zeros_like(nrg_image[:,:,:3], dtype=np.float32)\n",
    "        \n",
    "        for c in range(3):  # NRG channels\n",
    "            # Calculate stretch range: mean ± n_std * std\n",
    "            min_val = means[c] - n_std * stds[c]\n",
    "            max_val = means[c] + n_std * stds[c]\n",
    "            \n",
    "            # Apply stretching\n",
    "            channel_range = max_val - min_val\n",
    "            if channel_range > 0:\n",
    "                stretched_image[:, :, c] = (nrg_image[:, :, c] - min_val) / channel_range\n",
    "            else:\n",
    "                # If range is 0, just center around 0.5\n",
    "                stretched_image[:, :, c] = 0.5\n",
    "        \n",
    "        # Clip to [0, 1] range\n",
    "        stretched_image = np.clip(stretched_image, 0, 1)\n",
    "        return stretched_image\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, depths in dataloader:\n",
    "            images = images.to(device)\n",
    "            depths = depths.cpu().numpy()\n",
    "            \n",
    "            # Get predictions - handle DataParallel wrapper\n",
    "            if hasattr(model, 'module'):\n",
    "                depth_pred = model.module(images).cpu().numpy()\n",
    "            else:\n",
    "                depth_pred = model(images).cpu().numpy()\n",
    "            \n",
    "            # Denormalize images for visualization\n",
    "            images = images.cpu().numpy()\n",
    "            images = np.transpose(images, (0, 2, 3, 1))  # [B, C, H, W] -> [B, H, W, C]\n",
    "            \n",
    "            # Denormalize using your custom stats\n",
    "            images = images * np.array(stds) + np.array(means)\n",
    "            \n",
    "            # Create subplot with extra space for colorbar and title\n",
    "            fig, axes = plt.subplots(num_samples, 3, figsize=(15, num_samples*3))\n",
    "            \n",
    "            # Handle case where num_samples = 1\n",
    "            if num_samples == 1:\n",
    "                axes = axes.reshape(1, -1)\n",
    "            \n",
    "            for i in range(min(num_samples, images.shape[0])):\n",
    "                # Original image - apply mean/std stretching\n",
    "                denorm_image = images[i]\n",
    "                stretched_image = stretch_nrg_image_std(denorm_image, means, stds)\n",
    "                \n",
    "                axes[i, 0].imshow(stretched_image)\n",
    "                axes[i, 0].set_title(f'Input Image {i+1} (NRG Stretched)')\n",
    "                axes[i, 0].axis('off')\n",
    "                \n",
    "                # Ground truth depth\n",
    "                im1 = axes[i, 1].imshow(depths[i], cmap=forest_ht_cmap, norm=forest_ht_norm)\n",
    "                axes[i, 1].set_title(f'Ground Truth CHM\\n(max: {depths[i].max():.1f}m)')\n",
    "                axes[i, 1].axis('off')\n",
    "                \n",
    "                # Predicted depth  \n",
    "                im2 = axes[i, 2].imshow(depth_pred[i], cmap=forest_ht_cmap, norm=forest_ht_norm)\n",
    "                axes[i, 2].set_title(f'Predicted CHM\\n(max: {depth_pred[i].max():.1f}m)')\n",
    "                axes[i, 2].axis('off')\n",
    "                \n",
    "                # Add colorbar to each row's predicted image\n",
    "                cbar = plt.colorbar(im2, ax=axes[i, 2], shrink=0.8, aspect=20)\n",
    "                cbar.set_label('Canopy Height (m)', rotation=270, labelpad=20)\n",
    "                \n",
    "                # Customize colorbar ticks to show the boundary values\n",
    "                cbar.set_ticks([0, 0.001, 0.5, 1, 2, 3, 5, 10, 35])\n",
    "                cbar.set_ticklabels(['0', '0.001', '0.5', '1', '2', '3', '5', '10', '35'])\n",
    "                \n",
    "                # Calculate metrics for this sample\n",
    "                mse = np.mean((depths[i] - depth_pred[i])**2)\n",
    "                mae = np.mean(np.abs(depths[i] - depth_pred[i]))\n",
    "                print(f\"Sample {i+1} - MSE: {mse:.3f}, MAE: {mae:.3f}, RMSE: {np.sqrt(mse):.3f}\")\n",
    "            \n",
    "            # Use dynamic model description from config\n",
    "            model_name = data_config.get('current_model', 'model')\n",
    "            model_desc = f\"{model_name}_depth_estimation\"\n",
    "            \n",
    "            # Add suptitle with better positioning\n",
    "            plt.suptitle(f'DINOv3-{model_name.title()} Sample Predictions ({model_desc})', \n",
    "                        fontsize=16, y=0.98)  # Position higher with y parameter\n",
    "            \n",
    "            # Adjust layout with tighter spacing\n",
    "            plt.subplots_adjust(top=0.96, right=0.7, hspace=0.2, wspace=0.1)\n",
    "            \n",
    "            plt.savefig(f'dinov3_sample_predictions_{model_desc}.png', dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            break\n",
    "\n",
    "# Run the visualization with proper error handling\n",
    "print(\"\\nVisualizing individual sample predictions:\")\n",
    "\n",
    "# Check if we have a model to use\n",
    "if 'model_for_eval' in locals() and model_for_eval is not None:\n",
    "    # Use the model from evaluation\n",
    "    visualize_predictions_fixed(\n",
    "        model_for_eval, \n",
    "        test_loader_current, \n",
    "        DATA_CONFIG,  # Pass the config\n",
    "        num_samples=15, \n",
    "        device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    )\n",
    "    \n",
    "elif 'trained_model' in locals() and trained_model is not None:\n",
    "    # Use the trained model\n",
    "    model_to_use = trained_model.module if hasattr(trained_model, 'module') else trained_model\n",
    "    visualize_predictions_fixed(\n",
    "        model_to_use, \n",
    "        test_loader_current, \n",
    "        DATA_CONFIG,  # Pass the config\n",
    "        num_samples=15, \n",
    "        device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    # Try to load a model\n",
    "    try:\n",
    "        checkpoint_path = f'best_dinov3_{CURRENT_MODEL}_{DATA_CONFIG[\"data_name\"]}.pth'\n",
    "        loaded_model, _ = load_flexible_model(checkpoint_path)\n",
    "        visualize_predictions_fixed(\n",
    "            loaded_model, \n",
    "            test_loader_current, \n",
    "            DATA_CONFIG,  # Pass the config\n",
    "            num_samples=15, \n",
    "            device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"❌ No model available for visualization: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4196097a-f5ef-4791-a399-52bf2f3b00c0",
   "metadata": {},
   "source": [
    "## 7. Do Inference on Specitifc TIFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab27677-03f6-4b46-a1ca-0fd475c0fb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up based on data preprocessing\n",
    "\n",
    "# Add to your DATA_CONFIG or create new INFERENCE_CONFIG\n",
    "INFERENCE_CONFIG = {\n",
    "    'blue_min': 0,\n",
    "    'blue_max': 5681,\n",
    "    'nir_min': DATA_CONFIG['nir_min'],    \n",
    "    'nir_max': DATA_CONFIG['nir_max'],\n",
    "    'red_min': DATA_CONFIG['red_min'],\n",
    "    'red_max': DATA_CONFIG['red_max'],\n",
    "    'green_min': DATA_CONFIG['green_min'],\n",
    "    'green_max': DATA_CONFIG['green_max'],\n",
    "    'img_size': 64,\n",
    "    'overlap': 0.50,\n",
    "    'batch_size': 32,\n",
    "    'visualize_first': True\n",
    "}\n",
    "\n",
    "if DATA_CONFIG['data_name'][-3:] == 'rgb':\n",
    "    channels = [2,1,0]\n",
    "elif DATA_CONFIG['data_name'][-3:] == 'nrg':\n",
    "    channels = [3,2,1]\n",
    "else :\n",
    "    channels = [0,1,2,3]\n",
    "print(channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e71d146-74ca-471a-9281-a8711882ff91",
   "metadata": {},
   "outputs": [],
   "source": [
    "tif_list = [\n",
    "'/explore/nobackup/projects/above/misc/ABoVE_Shrubs/srlite/002m_2023/WV02_20200722_M1BS_10300100AB1FD100-sr-02m.tif',\n",
    "             '/explore/nobackup/projects/above/misc/ABoVE_Shrubs/srlite/002m_2023/WV03_20200704_M1BS_104001005EADB900-sr-02m.tif',\n",
    "             '/explore/nobackup/projects/above/misc/ABoVE_Shrubs/srlite/002m_2023/WV03_20190822_M1BS_104001005054F700-sr-02m.tif',\n",
    "             '/explore/nobackup/projects/above/misc/ABoVE_Shrubs/srlite/002m_2023/WV03_20170719_M1BS_104001002F7A2E00-sr-02m.tif',\n",
    "             '/explore/nobackup/projects/above/misc/ABoVE_Shrubs/srlite/002m_2023/WV02_20160817_M1BS_103001005B484100-sr-02m.tif',\n",
    "             '/explore/nobackup/projects/above/misc/ABoVE_Shrubs/srlite/002m_2023/WV02_20190820_M1BS_1030010099BF2F00-sr-02m.tif',\n",
    "             '/explore/nobackup/projects/above/misc/ABoVE_Shrubs/srlite/002m_2023/WV02_20110727_M1BS_103001000D914900-sr-02m.tif',\n",
    "             '/explore/nobackup/projects/above/misc/ABoVE_Shrubs/srlite/002m_2023/WV02_20190820_M1BS_10300100968FB300-sr-02m.tif',\n",
    "             '/explore/nobackup/projects/above/misc/ABoVE_Shrubs/srlite/002m_2023/WV02_20190820_M1BS_10300100954DE000-sr-02m.tif',\n",
    "             '/explore/nobackup/projects/above/misc/ABoVE_Shrubs/srlite/002m_2023/WV02_20120721_M1BS_103001001AB23900-sr-02m.tif']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e620d7e-c4a9-4b04-8550-7a828c88dc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to viz .tifs\n",
    "def display_array_as_rgb_mean(arr, channels=[2, 1, 0], figsize=(15, 10), title=\"RGB Image\", \n",
    "                             z_score_threshold=3, mask_negative=True, save_path=None):\n",
    "    \"\"\"\n",
    "    Display a multi-channel array as RGB image with mean±std normalization, handling negative values.\n",
    "    \n",
    "    Args:\n",
    "        arr: numpy array of shape (C, H, W)\n",
    "        channels: list of channel indices to use as [R, G, B] \n",
    "        figsize: figure size for matplotlib\n",
    "        title: title for the plot\n",
    "        z_score_threshold: number of standard deviations to include (default 3)\n",
    "        mask_negative: if True, mask values < 0 as NaN (transparent in display)\n",
    "        save_path: path to save the image (optional)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract the RGB channels\n",
    "    rgb_image = arr[channels].astype(np.float64)  # Use float64 to handle NaN\n",
    "    \n",
    "    # Transpose to (H, W, 3) for matplotlib\n",
    "    rgb_image = np.transpose(rgb_image, (1, 2, 0))\n",
    "    \n",
    "    print(f\"RGB image shape: {rgb_image.shape}\")\n",
    "       \n",
    "    # Handle negative values\n",
    "    negative_mask = rgb_image < 0\n",
    "    total_negative = np.sum(negative_mask)\n",
    "    total_pixels = rgb_image.size\n",
    "    \n",
    "    if total_negative > 0:\n",
    "        print(f\"\\nFound {total_negative:,} negative values ({100*total_negative/total_pixels:.2f}% of all pixel values)\")\n",
    "        \n",
    "        if mask_negative:\n",
    "            rgb_image[negative_mask] = np.nan\n",
    "            print(\"Negative values masked as NaN (will appear white/transparent)\")\n",
    "        else:\n",
    "            rgb_image[negative_mask] = 0\n",
    "            print(\"Negative values set to 0\")\n",
    "    else:\n",
    "        print(\"No negative values found\")\n",
    "    \n",
    "    # Print statistics and normalize each channel using mean±std\n",
    "    for i, ch in enumerate(channels):\n",
    "        channel_data = rgb_image[:, :, i]\n",
    "        valid_data = channel_data[~np.isnan(channel_data)]  # Exclude NaN values\n",
    "        \n",
    "        if len(valid_data) > 0:\n",
    "            print(f\"\\nChannel {ch+1} statistics:\")\n",
    "            print(f\"  Valid pixels: {len(valid_data):,} / {channel_data.size:,}\")\n",
    "            print(f\"  Min: {valid_data.min():.3f}, Max: {valid_data.max():.3f}\")\n",
    "            print(f\"  Mean: {valid_data.mean():.3f}, Std: {valid_data.std():.3f}\")\n",
    "            \n",
    "            # Mean±std normalization\n",
    "            mean_val = valid_data.mean()\n",
    "            std_val = valid_data.std()\n",
    "            lower_bound = mean_val - z_score_threshold * std_val\n",
    "            upper_bound = mean_val + z_score_threshold * std_val\n",
    "            \n",
    "            # Only normalize valid pixels\n",
    "            valid_mask = ~np.isnan(channel_data)\n",
    "            rgb_image[valid_mask, i] = (channel_data[valid_mask] - lower_bound) / (upper_bound - lower_bound)\n",
    "            \n",
    "            print(f\"  Normalization bounds: [{lower_bound:.3f}, {upper_bound:.3f}]\")\n",
    "        else:\n",
    "            print(f\"\\nChannel {ch+1}: No valid data after masking!\")\n",
    "    \n",
    "    # Clip valid values to [0, 1] range, but preserve NaN\n",
    "    for i in range(3):\n",
    "        valid_mask = ~np.isnan(rgb_image[:, :, i])\n",
    "        rgb_image[valid_mask, i] = np.clip(rgb_image[valid_mask, i], 0, 1)\n",
    "    \n",
    "    # Display the image\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(rgb_image)\n",
    "    plt.title(f'{title}\\nChannels: {[c+1 for c in channels]} (R,G,B) - Mean±{z_score_threshold}σ Normalization')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Image saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Return the arrays but don't print them\n",
    "    return rgb_image  # Return None instead of the arrays\n",
    "\n",
    "# Simple wrapper function\n",
    "def show_rgb_mean(arr):\n",
    "    \"\"\"Display RGB using mean±3σ normalization with negative masking\"\"\"\n",
    "    display_array_as_rgb_mean(\n",
    "        arr, \n",
    "        channels=[2, 1, 0],\n",
    "        title=\"RGB with Mean±3σ Normalization (Channels 3-2-1)\"\n",
    "    )\n",
    "\n",
    "# rgb_image = show_rgb_mean(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9179074c-d7aa-477a-8b2b-bf6f287350af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_dinov3_inference_pipeline_batch(tif_list, model, data_config, inference_config):\n",
    "    \"\"\"\n",
    "    Complete pipeline for DINOv3 inference on a list of large geospatial imagery files.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    tif_list : list\n",
    "        List of paths to TIF files to process\n",
    "    model : torch.nn.Module\n",
    "        Trained DINOv3 model\n",
    "    data_config : dict\n",
    "        Configuration containing data paths and normalization stats\n",
    "    inference_config : dict\n",
    "        Configuration containing inference parameters\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list : Results from inference on each file\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract configuration values\n",
    "    data_name = data_config['data_name']\n",
    "    means = data_config['means']\n",
    "    stds = data_config['stds']\n",
    "    \n",
    "    nir_min = inference_config['nir_min']\n",
    "    nir_max = inference_config['nir_max']\n",
    "    red_min = inference_config['red_min']\n",
    "    red_max = inference_config['red_max']\n",
    "    green_min = inference_config['green_min']\n",
    "    green_max = inference_config['green_max']\n",
    "    img_size = inference_config['img_size']\n",
    "    overlap = inference_config['overlap']\n",
    "    batch_size = inference_config['batch_size']\n",
    "    visualize_first = inference_config['visualize_first']\n",
    "    \n",
    "    print(\"🚀 Starting Batch DINOv3 Inference Pipeline\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = f\"./predict/{data_name}\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"Created folder: {output_dir}\")\n",
    "    else:\n",
    "        print(f\"Folder already exists: {output_dir}\")\n",
    "    \n",
    "    print(f\"Total TIF files to process: {len(tif_list)}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Set up model\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    results = []\n",
    "    output_h, output_w = None, None  # Initialize for model output dimensions\n",
    "    \n",
    "    for file_idx, tif_path in enumerate(tif_list, 1):\n",
    "        print(f\"\\n🔄 Processing TIF {file_idx}/{len(tif_list)}: {os.path.basename(tif_path)}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Load and preprocess imagery\n",
    "            print(\"📂 Step 1: Loading and preprocessing imagery...\")\n",
    "            \n",
    "            with rasterio.open(tif_path) as src:\n",
    "                print(f\"   Input shape: {src.shape}\")\n",
    "                print(f\"   Bands: {src.count}\")\n",
    "                \n",
    "                # Extract NIR, Red, Green channels (assuming bands 4, 3, 2)\n",
    "                nir_band = src.read(4)   # NIR\n",
    "                red_band = src.read(3)   # Red\n",
    "                green_band = src.read(2) # Green\n",
    "                \n",
    "                # Stack into NRG array (channels first)\n",
    "                nrg_array = np.stack([nir_band, red_band, green_band], axis=0)\n",
    "            \n",
    "            print(f\"   Extracted NRG shape: {nrg_array.shape}\")\n",
    "            \n",
    "            # Preprocess data\n",
    "            nrg_array = nrg_array.astype(np.float32)\n",
    "            nrg_array[nrg_array == -9999] = np.nan\n",
    "            \n",
    "            # Scale to 0-1 range first (like in training)\n",
    "            nrg_array[0] = (nrg_array[0] - nir_min) / (nir_max - nir_min)\n",
    "            nrg_array[1] = (nrg_array[1] - red_min) / (red_max - red_min) \n",
    "            nrg_array[2] = (nrg_array[2] - green_min) / (green_max - green_min)\n",
    "            \n",
    "            print(\"nir_mean: \", np.nanmean(nrg_array[0]), \" red_mean: \", np.nanmean(nrg_array[1]), \" green_mean: \", np.nanmean(nrg_array[2]))            \n",
    "            \n",
    "            # Clip to ensure 0-1 range\n",
    "            nrg_array = np.clip(nrg_array, 0, 1)\n",
    "            \n",
    "            # Normalize using training statistics\n",
    "            nrg_array[0] = (nrg_array[0] - means[0]) / stds[0]\n",
    "            nrg_array[1] = (nrg_array[1] - means[1]) / stds[1]\n",
    "            nrg_array[2] = (nrg_array[2] - means[2]) / stds[2]\n",
    "            \n",
    "            print(f\"   ✓ Preprocessing complete\")\n",
    "            print(f\"   NaN pixels: {np.isnan(nrg_array).sum():,}\")\n",
    "            \n",
    "            # Step 2: Set up sliding window inference\n",
    "            print(f\"🧠 Step 2: Running DINOv3 inference...\")\n",
    "            \n",
    "            # Transpose to (H, W, C) for tiler\n",
    "            xraster = nrg_array.transpose(1, 2, 0)\n",
    "            \n",
    "            # Set up image tiler\n",
    "            tiler_image = Tiler(\n",
    "                data_shape=xraster.shape,\n",
    "                tile_shape=(img_size, img_size, 3),\n",
    "                channel_dimension=-1,\n",
    "                overlap=overlap,\n",
    "                mode='reflect'\n",
    "            )\n",
    "            \n",
    "            # Get model output dimensions (only for first file)\n",
    "            if file_idx == 1:\n",
    "                test_input = torch.randn(1, 3, img_size, img_size).to(device)\n",
    "                with torch.no_grad():\n",
    "                    test_output = model(test_input)\n",
    "                    output_h, output_w = test_output.shape[1], test_output.shape[2]\n",
    "                print(f\"   Model output size: {output_h}x{output_w}\")\n",
    "            \n",
    "            # Set up merger for combining tile predictions\n",
    "            tiler_mask = Tiler(\n",
    "                data_shape=(xraster.shape[0], xraster.shape[1], 1),\n",
    "                tile_shape=(output_h, output_w, 1),\n",
    "                channel_dimension=-1,\n",
    "                overlap=overlap,\n",
    "                mode='reflect'\n",
    "            )\n",
    "            merger = Merger(tiler=tiler_mask, window='triang')\n",
    "            \n",
    "            # Process tiles in batches\n",
    "            total_tiles = len(tiler_image)\n",
    "            print(f\"   Processing {total_tiles} tiles...\")\n",
    "            \n",
    "            tile_count = 0\n",
    "            for batch_id, batch_i in tiler_image(xraster, batch_size=batch_size):\n",
    "                actual_batch_size = len(batch_i)\n",
    "                \n",
    "                # Prepare input batch\n",
    "                input_batch = batch_i.transpose(0, 3, 1, 2).astype('float32')\n",
    "                input_batch_tensor = torch.from_numpy(input_batch).to(device)\n",
    "                input_batch_tensor = torch.nan_to_num(input_batch_tensor, nan=0.0)\n",
    "                \n",
    "                # Run inference\n",
    "                with torch.no_grad():\n",
    "                    y_batch = model(input_batch_tensor)\n",
    "                    y_batch_numpy = y_batch.cpu().numpy()\n",
    "                    \n",
    "                    # Format for merger (B, H, W, C)\n",
    "                    if len(y_batch_numpy.shape) == 3:\n",
    "                        formatted_output = np.expand_dims(y_batch_numpy, axis=-1)\n",
    "                    else:\n",
    "                        formatted_output = y_batch_numpy\n",
    "                \n",
    "                # Add predictions to merger\n",
    "                for j in range(actual_batch_size):\n",
    "                    tile_id = batch_id * batch_size + j\n",
    "                    merger.add(tile_id, formatted_output[j])\n",
    "                \n",
    "                tile_count += actual_batch_size\n",
    "            \n",
    "            # Merge tile predictions\n",
    "            print(\"   🔄 Merging tiles...\")\n",
    "            predictions_raw = merger.merge(unpad=True)\n",
    "            predictions_meters = np.squeeze(predictions_raw)\n",
    "            \n",
    "            print(f\"   ✓ Inference complete\")\n",
    "            print(f\"   Final prediction shape: {predictions_meters.shape}\")\n",
    "            \n",
    "            # Step 3: Post-process predictions\n",
    "            print(f\"🔧 Step 3: Post-processing predictions...\")\n",
    "            \n",
    "            # Restore original NoData locations\n",
    "            original_nodata = np.isnan(nrg_array[0])\n",
    "            predictions_meters[original_nodata] = -9999\n",
    "            \n",
    "            # Convert to decimeters (int16 for storage efficiency)\n",
    "            predictions_decimeters = convert_predictions_to_decimeters_int16(predictions_meters)\n",
    "            print(f\"   ✓ Converted to decimeters (int16)\")\n",
    "            \n",
    "            # Step 4: Save as GeoTIFF\n",
    "            print(f\"💾 Step 4: Saving GeoTIFF...\")\n",
    "            \n",
    "            # Generate output filename\n",
    "            base_name = os.path.basename(tif_path)\n",
    "            if len(base_name) >= 46:\n",
    "                string = base_name[-46:-10]\n",
    "            else:\n",
    "                string = base_name[:-4]\n",
    "            output_tif = os.path.join(output_dir, f'{string}sr-02m.chm.tif')\n",
    "            \n",
    "            save_predictions_as_geotiff(predictions_decimeters, tif_path, output_tif)\n",
    "            \n",
    "            # Step 5: Optional visualization\n",
    "            if (file_idx == 1 and visualize_first) or (visualize_first and file_idx <= 3):\n",
    "                print(f\"📊 Step 5: Generating visualization for file {file_idx}...\")\n",
    "                \n",
    "                plt.figure(figsize=(20, 6))\n",
    "                \n",
    "                # Original NRG composite\n",
    "                plt.subplot(1, 3, 1)\n",
    "                nrg_display = nrg_array.transpose(1, 2, 0)\n",
    "                display_step = max(1, min(nrg_display.shape[0], nrg_display.shape[1]) // 1000)\n",
    "                nrg_sub = nrg_display[::display_step, ::display_step, :]\n",
    "                \n",
    "                # Normalize for display\n",
    "                nrg_norm = np.zeros_like(nrg_sub)\n",
    "                for ch in range(3):\n",
    "                    channel = nrg_sub[:, :, ch]\n",
    "                    valid = ~np.isnan(channel)\n",
    "                    if valid.any():\n",
    "                        vmin, vmax = np.nanpercentile(channel[valid], [2, 98])\n",
    "                        if vmax > vmin:\n",
    "                            nrg_norm[:, :, ch] = np.clip((channel - vmin)/(vmax - vmin), 0, 1)\n",
    "                \n",
    "                plt.imshow(nrg_norm)\n",
    "                plt.title(f'NRG Composite\\n{os.path.basename(tif_path)}')\n",
    "                plt.axis('off')\n",
    "                \n",
    "                # Predicted CHM\n",
    "                plt.subplot(1, 3, 2)\n",
    "                pred_display = predictions_meters.copy()\n",
    "                pred_display[pred_display == -9999] = np.nan\n",
    "                pred_sub = pred_display[::display_step, ::display_step]\n",
    "                \n",
    "                valid_pred = pred_display[~np.isnan(pred_display)]\n",
    "                if len(valid_pred) > 0:\n",
    "                    vmin, vmax = np.nanpercentile(valid_pred, [1, 99])\n",
    "                    im1 = plt.imshow(pred_sub, cmap=forest_ht_cmap, norm=forest_ht_norm)\n",
    "                    plt.colorbar(im1, label='Height (m)')\n",
    "                else:\n",
    "                    plt.imshow(pred_sub, cmap=forest_ht_cmap, norm=forest_ht_norm)\n",
    "                \n",
    "                plt.title('Predicted CHM (meters)')\n",
    "                plt.axis('off')\n",
    "                \n",
    "                # Statistics histogram\n",
    "                plt.subplot(1, 3, 3)\n",
    "                if len(valid_pred) > 0:\n",
    "                    plt.hist(valid_pred, bins=50, alpha=0.7, edgecolor='black')\n",
    "                    plt.xlabel('Predicted Height (m)')\n",
    "                    plt.ylabel('Frequency')\n",
    "                    plt.title(f'Height Distribution\\nMean: {valid_pred.mean():.2f}m')\n",
    "                    plt.grid(True, alpha=0.3)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'{output_dir}/visualization_file_{file_idx:03d}.png', \n",
    "                           dpi=150, bbox_inches='tight')\n",
    "                plt.show()\n",
    "            \n",
    "            # Add to results\n",
    "            results.append(predictions_meters)\n",
    "            \n",
    "            # Print summary for this file\n",
    "            valid_pixels = (predictions_meters != -9999).sum()\n",
    "            nodata_pixels = (predictions_meters == -9999).sum()\n",
    "            \n",
    "            print(f\"✅ Completed TIF {file_idx}/{len(tif_list)}\")\n",
    "            print(f\"   📊 Valid predictions: {valid_pixels:,} pixels\")\n",
    "            print(f\"   🚫 NoData pixels: {nodata_pixels:,} pixels\")\n",
    "            \n",
    "            if valid_pixels > 0:\n",
    "                valid_heights = predictions_meters[predictions_meters != -9999]\n",
    "                print(f\"   🌲 Height range: {valid_heights.min():.2f}m to {valid_heights.max():.2f}m\")\n",
    "                print(f\"   📈 Mean height: {valid_heights.mean():.2f}m ± {valid_heights.std():.2f}m\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {tif_path}: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            results.append(None)\n",
    "            continue\n",
    "    \n",
    "    # Print final summary\n",
    "    successful_files = sum(1 for r in results if r is not None)\n",
    "    failed_files = len(results) - successful_files\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"⭐ BATCH INFERENCE COMPLETE ⭐\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"📁 Output directory: {output_dir}\")\n",
    "    print(f\"✅ Successfully processed: {successful_files}/{len(tif_list)} files\")\n",
    "    if failed_files > 0:\n",
    "        print(f\"❌ Failed: {failed_files} files\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb27602-0b00-44ba-9ccd-788ea070e104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tif to decimeters and make 0 min\n",
    "def convert_predictions_to_decimeters_int16(predictions):\n",
    "    \"\"\"\n",
    "    Convert predictions from meters to decimeters, round to nearest integer, \n",
    "    and convert to int16 data type. Preserves -9999 NoData values.\n",
    "    \n",
    "    Parameters:\n",
    "    predictions: numpy array with height predictions in meters (may contain -9999)\n",
    "    \n",
    "    Returns:\n",
    "    converted_predictions: numpy array of int16 type with values in decimeters\n",
    "    \"\"\"\n",
    "    # Create NoData mask\n",
    "    nodata_mask = predictions == -9999\n",
    "    \n",
    "    # Work with valid data only\n",
    "    valid_predictions = predictions.copy()\n",
    "    valid_predictions[nodata_mask] = 0  # Temporarily set to 0 for processing\n",
    "    \n",
    "    # Multiply by 10 to convert meters to decimeters\n",
    "    decimeters = valid_predictions * 10\n",
    "    \n",
    "    # Round to nearest integer\n",
    "    rounded = np.round(decimeters)\n",
    "    \n",
    "    # Convert to int16\n",
    "    converted_predictions = rounded.astype(np.int16)\n",
    "\n",
    "    # Make all negative vlaues = 0\n",
    "    converted_predictions[converted_predictions < 0] = 0 \n",
    "    \n",
    "    # Restore NoData values\n",
    "    converted_predictions[nodata_mask] = -9999\n",
    "    \n",
    "    print(f\"Converted to decimeters. NoData pixels preserved: {np.sum(nodata_mask):,}\")\n",
    "    \n",
    "    return converted_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07073cd1-d858-4b95-9578-8fbbd49d8076",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compress and Save Tif\n",
    "def save_predictions_as_geotiff(predictions_decimeters, reference_tif_path, output_path):\n",
    "    \"\"\"\n",
    "    Save predictions as a GeoTIFF with matching geotransform and projection from reference,\n",
    "    with LZW compression, NoData value of -9999, and metadata about units.\n",
    "    \n",
    "    Parameters:\n",
    "    predictions_decimeters: numpy array of int16 predictions in decimeters\n",
    "    reference_tif_path: path to the original .tif file for geospatial reference\n",
    "    output_path: path where to save the output GeoTIFF\n",
    "    \"\"\"\n",
    "    # Open the reference dataset to get geospatial information\n",
    "    ref_ds = gdal.Open(reference_tif_path)\n",
    "    if ref_ds is None:\n",
    "        raise ValueError(f\"Could not open reference file: {reference_tif_path}\")\n",
    "    \n",
    "    # Get geospatial information from reference\n",
    "    geotransform = ref_ds.GetGeoTransform()\n",
    "    projection = ref_ds.GetProjection()\n",
    "    \n",
    "    # Get dimensions\n",
    "    height, width = predictions_decimeters.shape\n",
    "    \n",
    "    # Replace NaN values with -9999 (NoData value)\n",
    "    output_array = predictions_decimeters.copy()\n",
    "    if np.issubdtype(predictions_decimeters.dtype, np.floating):\n",
    "        output_array = np.where(np.isnan(predictions_decimeters), -9999, predictions_decimeters)\n",
    "    output_array = output_array.astype(np.int16)\n",
    "    \n",
    "    # Create the output dataset\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    \n",
    "    # Create dataset with LZW compression\n",
    "    out_ds = driver.Create(\n",
    "        output_path, \n",
    "        width, \n",
    "        height, \n",
    "        1,  # number of bands\n",
    "        gdal.GDT_Int16,  # data type\n",
    "        options=['COMPRESS=LZW', 'TILED=YES']  # LZW compression + tiling for efficiency\n",
    "    )\n",
    "    \n",
    "    if out_ds is None:\n",
    "        raise ValueError(f\"Could not create output file: {output_path}\")\n",
    "    \n",
    "    # Set geospatial information\n",
    "    out_ds.SetGeoTransform(geotransform)\n",
    "    out_ds.SetProjection(projection)\n",
    "    \n",
    "    # Get the band and write data\n",
    "    band = out_ds.GetRasterBand(1)\n",
    "    band.WriteArray(output_array)\n",
    "    \n",
    "    # Set NoData value\n",
    "    band.SetNoDataValue(-9999)\n",
    "    \n",
    "    # Add metadata about units\n",
    "    band.SetDescription(\"Canopy Height Model - Values in decimeters\")\n",
    "    band.SetMetadataItem(\"UNITS\", \"decimeters\")\n",
    "    band.SetMetadataItem(\"DESCRIPTION\", \"Predicted canopy heights in decimeters (1 meter = 10 decimeters)\")\n",
    "    \n",
    "    # Add dataset-level metadata\n",
    "    out_ds.SetMetadataItem(\"PROCESSING\", \"DINOv3 model prediction\")\n",
    "    out_ds.SetMetadataItem(\"NODATA_VALUE\", \"-9999\")\n",
    "    out_ds.SetMetadataItem(\"UNITS\", \"decimeters\")\n",
    "    \n",
    "    # Flush and close\n",
    "    band.FlushCache()\n",
    "    out_ds.FlushCache()\n",
    "    band = None\n",
    "    out_ds = None\n",
    "    ref_ds = None\n",
    "    \n",
    "    print(f\"Successfully saved predictions to: {output_path}\")\n",
    "    print(f\"- Compression: LZW\")\n",
    "    print(f\"- NoData value: -9999\") \n",
    "    print(f\"- Units: decimeters\")\n",
    "    print(f\"- Data type: int16\")\n",
    "    print(f\"- Dimensions: {height} x {width}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf962f8-cfa5-4d99-92c4-244869833357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage:\n",
    "if model_for_eval is not None:\n",
    "    print(model_filename)\n",
    "    batch_results = complete_dinov3_inference_pipeline_batch(\n",
    "        tif_list=tif_list,\n",
    "        model=model_for_eval,\n",
    "        data_config=DATA_CONFIG,\n",
    "        inference_config=INFERENCE_CONFIG\n",
    "    )\n",
    "else:\n",
    "    print(\"❌ No model available for inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ac686b-591f-4fed-8669-6f24e154951f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ab09dc-8796-464a-99bf-4d52ae075410",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DEV Kernel",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
